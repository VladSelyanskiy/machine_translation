Yes, you’re right. So because locally weighted regression is a non-parametric algorithm every time you make a prediction you need to fit theta to your entire training set again. So you’re actually right. If you have a very large training set then this is a somewhat expensive algorithm to use. Because every time you want to make a prediction you need to fit a straight line to a huge data set again. Turns out there are algorithms that – turns out there are ways to make this much more efficient for large data sets as well. So don’t want to talk about that. If you’re interested, look up the work of Andrew Moore on KD-trees. He, sort of, figured out ways to fit these models much more efficiently.