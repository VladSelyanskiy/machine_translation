Да, вы правы. Поскольку локально взвешенная регрессия — это непараметрический алгоритм, каждый раз, когда вы делаете прогноз, вам нужно заново подгонять тета ко всему обучающему набору. Так что вы действительно правы. Если у вас очень большой обучающий набор, то этот алгоритм довольно затратен. Потому что каждый раз, когда вы хотите сделать прогноз, вам нужно заново подгонять прямую линию к огромному набору данных. Оказывается, есть алгоритмы, которые… оказывается, есть способы сделать это гораздо эффективнее и для больших наборов данных. Так что не хочу об этом говорить. Если вам интересно, почитайте работу Эндрю Мура о деревьях Короля-Размера. Он, как бы, придумал способы гораздо более эффективного подгона этих моделей.