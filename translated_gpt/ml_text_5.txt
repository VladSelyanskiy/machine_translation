Да, вы правы. Поскольку локально взвешенная регрессия является непараметрическим алгоритмом, каждый раз, когда вы делаете прогноз, вам нужно снова подгонять тета к всему вашему обучающему набору. Так что вы правы. Если у вас очень большой обучающий набор, то этот алгоритм будет несколько затратным в использовании. Потому что каждый раз, когда вы хотите сделать прогноз, вам нужно снова подогнать прямую линию к огромному набору данных. Оказывается, есть алгоритмы, которые - оказывается, есть способы сделать это намного более эффективным и для больших наборов данных. Но сейчас не будем об этом говорить. Если вам интересно, посмотрите работу Эндрю Мура о KD-деревьях. Он, вроде как, нашел способы, как подогнать эти модели намного эффективнее.
